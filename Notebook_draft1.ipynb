{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5378955f",
   "metadata": {},
   "source": [
    "# Unsupervised Learning Predict\n",
    "Â© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "*   Thomas Kenyon\n",
    "*   Name\n",
    "*   Name\n",
    "*   Name\n",
    "*   Name\n",
    "---\n",
    "\n",
    "### Honour Code\n",
    "\n",
    "We, Team 8, confirm - by submitting this document - that the solutions in this notebook are a result of our own work and that we abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "---\n",
    "\n",
    "### Predict Overview: Movie Recommender\n",
    "\n",
    "--Insert description and context here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2cc4e4",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>\n",
    "\n",
    "<a href=#eight>8. Appendix</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d5ae13",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42d2a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our regular old heroes \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp # <-- The sister of Numpy, used in our code for numerical efficientcy. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Entity featurization and similarity computation\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Libraries used during sorting procedures.\n",
    "import operator # <-- Convienient item retrieval during iteration \n",
    "import heapq # <-- Efficient sorting of large lists\n",
    "\n",
    "# Imported for our sanity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af221e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.8.3.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cufflinks as cf\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c028571",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62474d3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'genome_scores.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18216/1137839956.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgenome_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'genome_scores.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mgenome_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'genome_tags.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mimdb_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'imdb_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlinks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'links.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmovies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'movies.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'genome_scores.csv'"
     ]
    }
   ],
   "source": [
    "genome_scores = pd.read_csv('genome_scores.csv')\n",
    "genome_tags = pd.read_csv('genome_tags.csv')\n",
    "imdb_data = pd.read_csv('imdb_data.csv')\n",
    "links = pd.read_csv('links.csv')\n",
    "movies = pd.read_csv('movies.csv')\n",
    "tags = pd.read_csv('tags.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf974a",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0701c7d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'genome_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18216/544595891.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgenome_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mgenome_tags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'genome_scores' is not defined"
     ]
    }
   ],
   "source": [
    "genome_scores.isnull().sum()\n",
    "genome_tags.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3939d6ff",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Data Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "\n",
    "Prior to training a model with any sort of data, it is essential to re-engineer it. This is to ensure that the data is in a consistent form with no missing values, incorrect data types or just plain incorrect data. For structured,  numeric data, this entails scaling values, filling in missing values and typecasting any non-numeric data to numeric form. Non-numeric data, such as this database is often unstructured and consists of text data. This type of data is not easily interpretable by computers. Computers work with 1s and 0s, not letters and words. Therefore it must be converted into a form that is interpretable, and therefore, numeric data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a530ade2",
   "metadata": {},
   "source": [
    "### Content-based Recommender\n",
    "\n",
    "---\n",
    "First thing, we need to merge the movies dataframe with the imdb_data dataframe, these two dataframes contain most of the information we need to construct our recommender.\n",
    "\n",
    "As you can see, for each movie we now have multiple columns that contain useful information about each movie:\n",
    "* genres\n",
    "* title_cast\n",
    "* director\n",
    "* plot_keywords\n",
    "\n",
    "There are more columns that could be useful, such as budget, but lets stick to these for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae741755",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined2 = movies.merge(imdb_data,  how='left', on = 'movieId')\n",
    "joined2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaabeee",
   "metadata": {},
   "source": [
    "There is a lot of actors names for each movie in the title_cast column! Since we're going to ultimately be vectorizing all of this data, we should ask ourselves if we really need all the actors for each movie listed in this column. For the sake of minimizing the number of features in our vectorized dataset, lets reduce this to the top 3 actors. The first 3 actors in this column seem to be the top-billed/most prominent cast for each film, so it's likely that they're the most important anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d67799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_3(actors):\n",
    "  s = []\n",
    "  s = [actor for actor in actors if len(s) < 3]\n",
    "  s2 = s[:3]\n",
    "  return '|'.join([actor for actor in s2])\n",
    "\n",
    "# x = ['Rhys Ifans', 'Tim Allen', 'Don Rickles', 'Crispin Glover', 'Christian McKay']\n",
    "# print(first_3(x))\n",
    "\n",
    "\n",
    "joined2['title_cast'] = joined2['title_cast'].apply(lambda x: str(x).split('|'))\n",
    "joined2['title_cast'] = joined2['title_cast'].apply(first_3)\n",
    "joined2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b96e67d",
   "metadata": {},
   "source": [
    "One problem with this dataset are the number of nan values. Most of these are actual np.nan null values, however some of them are actually 'nan' strings. Sneaky! We'll need to convert all these cryptic nan values to real nan values and then imput something whenever they occur, since null values cannot be passed to a vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03dd91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan(field):\n",
    "  '''Some NaN values are being stored as strings, so fillna wont work'''\n",
    "  if not isinstance(field, str):\n",
    "    return np.nan\n",
    "  f2 = field.strip().lower()\n",
    "  if 'nan' in f2:\n",
    "    return np.nan\n",
    "  else:\n",
    "    return field\n",
    "\n",
    "joined3 = joined2.copy()\n",
    "joined3['title_cast'] = joined3['title_cast'].apply(check_nan)\n",
    "joined3['plot_keywords'] = joined3['plot_keywords'].apply(check_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4641c285",
   "metadata": {},
   "source": [
    "Lets replace all nan values with empty strings. This means that there will be a lot of movies with a lot of empty metadata columns, but hopefully when we include the genome tag dataset this will become less of an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bcff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined3['director'].fillna('', inplace = True)\n",
    "joined3['title_cast'].fillna('', inplace = True)\n",
    "joined3['runtime'].fillna(0, inplace = True)\n",
    "joined3['budget'].fillna(0, inplace = True)\n",
    "joined3['plot_keywords'].fillna('', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7cb532",
   "metadata": {},
   "source": [
    "Creating a word soup column. This column has all the text data in it that we will vectorize. Think of all the words/names in these columns becoming tags. I've added director 3 times so that it has more weighting. IE, if someone likes a movie like Inception, then they are likely to enjoy other movies directer by Christopher Nolan. This is quite a janky way of altering the weightings of specific tags, but creating a word soup like this does simplify the vectorizing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d963c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined3['soup'] = joined3['genres'] + '|' + joined3['director'] + '|' + joined3['director'] + '|' + joined3['director'] + '|' + joined3['plot_keywords'] + '|'+ joined3['title_cast']\n",
    "joined3.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d314bd51",
   "metadata": {},
   "source": [
    "The movie titles contain their release year. We'll leave those in to keep things compatible with the streamlit app. But we'll also add the years to their own column. Just in case we decide to use this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d9371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year(title):\n",
    "  s = title.split()\n",
    "  year = s[-1]\n",
    "  if '(' in year and ')' in year:\n",
    "    year2 = year[1:-1]\n",
    "    return year2\n",
    "  else:\n",
    "    return np.nan\n",
    "  \n",
    "joined3['year'] = joined3['title'].apply(get_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8dfde2",
   "metadata": {},
   "source": [
    "Filling in nan values in the year column with 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c4aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined3['year'].fillna('0', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b88f6",
   "metadata": {},
   "source": [
    "#### What to do about the genome tag data?\n",
    "\n",
    "---\n",
    "The genome tag data consists of 1128 tags assigned to the movies in the movie database. What is handy about this data is that there are no null values. 1128 standardized tags used across all movies is also useful for the purposes of vectorization, at most, we'll only be adding 1128 new features. This is very efficient considering how much data there is! One other thing, each tag for each movie is assigned a relevance value. Values closer to zero mean that particular tag is not related to that movie, a relevance closer to 1 means that a tag is related to a movie.\n",
    "\n",
    "---\n",
    "How would we add these tags to our word soup data while also accounting for the extra data contained in the relevance column? The option we've gone for here is again quite janky, tags with a relevance >= 0.5 will be added to a particular movie's word soup once, while tags with a relevance >= 0.8 will be added twice, thus increasing their weighting in the same way adding the director of a movie multiple times to its word soup increases weighting.\n",
    "\n",
    "---\n",
    "Word of warning, we could not find a straightforward way to write this code more efficiently, on a relatively new laptop with an 11th gen i5 processer the cell below takes 2h30m to run. Fortunately, it only needs to be run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41a6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def get_genome_tags(movieid):\n",
    "    scores = genome_scores.loc[genome_scores['movieId'] == movieid]\n",
    "    output = []\n",
    "    output2 = ''\n",
    "    for index, row in scores.iterrows(): # itering through a dataframe is heresy but we're taking the janky route here\n",
    "        tag = genome_tags.loc[genome_tags['tagId'] == row['tagId']].values[0][1]\n",
    "        relevance = row['relevance']\n",
    "        if relevance >= 0.5:\n",
    "            output += [tag]\n",
    "        if relevance >= 0.80:\n",
    "            output += [tag]\n",
    "        output2 = '|'.join([x for x in output])\n",
    "        #print(output2)\n",
    "    return output2\n",
    "\n",
    "\n",
    "# joined3['genome'] = joined3['movieId'].apply(get_genome_tags)\n",
    "joined4 = joined3.copy()\n",
    "# joined4\n",
    "joined4['genome'] = joined4['movieId'].apply(get_genome_tags)\n",
    "joined4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d48ac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined4['soup'] = joined4['soup'] + '|' + joined4['genome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a0d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined4.drop('genome', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af1e2a8",
   "metadata": {},
   "source": [
    "Uncomment and run this cell below to load the output dataframe from code 3 cells above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e95044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined4 = pd.read_csv('joined_df_soup_genome3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa3417",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined4['soup'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd01b83",
   "metadata": {},
   "source": [
    "Great! That looks like a pretty decent collection of tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e55b7a7",
   "metadata": {},
   "source": [
    "#### Time to vectorize\n",
    "We've got a soup column that now contains all the movie information we want. It's time to vectorize! One thing to note, we'll need to be careful to ensure that the vectorizer splits our word soup into the correct 'tokens'. We don't want tokens to be generate at each whitespace, Morgan Freeman is one person (or God?) so we don't want our vectorizer to split his name into two tokens. We'll avoid this by using a special regex pattern that'll split tokens when it encounters the '|' symbol, since that is what we've been using to seperate our terms in our word soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67109b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(analyzer='word',\n",
    "                     min_df=4, max_df=0.5, max_features=100000, token_pattern='[A-Za-z .]{3,50}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d4ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_soup = tf.fit_transform(joined4['soup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5471c347",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c8bfe",
   "metadata": {},
   "source": [
    "This vectorized object is a sparse matrix, if we wanted to use as little memory as possible we'd keep it sparse, but for simplicity's sake we're going to convert it to a dense array and then to a dataframe so we can use the movie titles as the dataframe's index. This will definitely use far more memory, but it'll make our code easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb9bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tf_soup.toarray(),  columns=tf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be701bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.index = joined4['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a96f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec9b7d",
   "metadata": {},
   "source": [
    "A few movies in this dataset are actually duplicated, lets remove those duplicates now before attempting PCA or any cosine similarity calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c436d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = tfidf_df[~tfidf_df.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed746747",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b61f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc5bf3",
   "metadata": {},
   "source": [
    "### PCA time\n",
    "\n",
    "---\n",
    "Okay, so we've got an enormous dataframe with 62325 rows and 6526 columns (features). That is a huge dataframe! Not just to store and manipulate but also to perform calculations on. It's likely that many of these features are not that important, ie, they don't contain much information about each observation and don't explain much of the variation in the dataset. We'll use Principal Component analysis here to reduce the number of features in our dataset while having a minimal effect on the the quality of the dataset.\n",
    "\n",
    "---\n",
    "Another reason we converted our vectorized soup data to a dataframe is that Sklearn's PCA implementation doesn't support sparse matrices as input!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0484aa",
   "metadata": {},
   "source": [
    "DO NOT run the cell below. IT takes ~12 minutes to run and uses about 13GB of memory. The image below is it's output and has been saved from a previous run and added here for convenience purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a960fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# fit the PCA model to our data and apply the dimensionality reduction \n",
    "prin_comp = pca.fit_transform(tfidf_df)\n",
    "\n",
    "# create a dataframe containing the principal components\n",
    "pca_df = pd.DataFrame(data = prin_comp)\n",
    "\n",
    "# plot line graph of cumulative variance explained \n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62d4f51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://github.com/ethanmacrae/unsupervised-predict-streamlit-template/blob/master/resources/imgs/Cumulative_explained_variance.png?raw=true\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "Image(url= \"https://github.com/ethanmacrae/unsupervised-predict-streamlit-template/blob/master/resources/imgs/Cumulative_explained_variance.png?raw=true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf080f",
   "metadata": {},
   "source": [
    "Amazing! From the graph above, we can see that the majority of variation between observations can be explained by a fraction of the features. It appears that if we were to keep the 1000 most significant principal components we'd still be able to explain ~90% of the variation. Not only will that significantly reduce the memory footprint of our dataset, it'll also drastically speed up movie prediction times.\n",
    "\n",
    "---\n",
    "Lets now confirm that the first 1000 principal components really do explain most of the variance and then store them in a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76221416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create PCA object with n_components set to 1000\n",
    "pca_reg = PCA(n_components=1000)\n",
    "\n",
    "# fit the PCA model to our data and apply the dimensionality reduction \n",
    "PCA_array = pca_reg.fit_transform(tfidf_df)\n",
    "\n",
    "# confirm the number of components\n",
    "pca_reg.n_components_\n",
    "\n",
    "\n",
    "\n",
    "pca_reg.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546ad205",
   "metadata": {},
   "source": [
    "Storing the PCA data as a numpy array proved to be the most efficient (better than a dataframe). This .npy file is what is loaded into streamlit and used to make content-based recommendations\n",
    "\n",
    "---\n",
    "Since this data doesn't contain movie titles, we're also going to save a dataframe containing just the movie titles as a separate csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95b4c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import save\n",
    "save('PCA1000features.npy', PCA_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540432fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df = tfidf_df.index\n",
    "titles_df = pd.DataFrame(titles_df.values, columns=['title'])\n",
    "titles_df.to_csv('titles_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d541f0",
   "metadata": {},
   "source": [
    "Assuming you haven't run the the PCA code above for obvious reasons, you can run the cell below to load the two files that were saved in the two cells above. Then we'll assign the movie titles as indexes in the PCA dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd51fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "# load array\n",
    "PCA_array = load('PCA1000features.npy')\n",
    "titles = pd.read_csv('titles_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152aa97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_df = pd.DataFrame(PCA_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32b4507",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_df.index = titles['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21906981",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dd5c31",
   "metadata": {},
   "source": [
    "Below is our function to generate contant-based recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ebbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def content_model(movie_list,top_n=10):\n",
    "    recommended_movies = []\n",
    "    sims1 = cosine_similarity(PCA_df.loc[movie_list[0]].values.reshape(1, -1), PCA_df)\n",
    "    sims2 = cosine_similarity(PCA_df.loc[movie_list[1]].values.reshape(1, -1), PCA_df)\n",
    "    sims3 = cosine_similarity(PCA_df.loc[movie_list[2]].values.reshape(1, -1), PCA_df)\n",
    "    sims1_df = pd.DataFrame(sims1.T, index=PCA_df.index,columns=['similarity_score'])\n",
    "    sims2_df = pd.DataFrame(sims2.T, index=PCA_df.index,columns=['similarity_score'])\n",
    "    sims3_df = pd.DataFrame(sims3.T, index=PCA_df.index,columns=['similarity_score'])\n",
    "    del sims1,sims2,sims3\n",
    "    sims1_df.drop(movie_list[0], inplace=True)\n",
    "    sims2_df.drop(movie_list[1], inplace=True)\n",
    "    sims3_df.drop(movie_list[2], inplace=True)\n",
    "    sims1_df_sorted = sims1_df.sort_values(by='similarity_score', ascending=False)\n",
    "    sims2_df_sorted = sims2_df.sort_values(by='similarity_score', ascending=False)\n",
    "    sims3_df_sorted = sims3_df.sort_values(by='similarity_score', ascending=False)\n",
    "    del sims1_df,sims2_df,sims3_df\n",
    "    sims1_df_sorted = sims1_df_sorted.head(100)\n",
    "    sims2_df_sorted = sims2_df_sorted.head(100)\n",
    "    sims3_df_sorted = sims3_df_sorted.head(100)\n",
    "    final = sims1_df_sorted.append([sims2_df_sorted, sims3_df_sorted])\n",
    "    final = final[~final.index.duplicated(keep='first')]\n",
    "    final.sort_values(by='similarity_score', ascending=False, inplace=True)\n",
    "    recommended_movies = final.head(top_n).index\n",
    "    return recommended_movies\n",
    "    \n",
    "print(content_model(['Moana (2016)', 'Toy Story (1995)', 'Ice Age (2002)']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
