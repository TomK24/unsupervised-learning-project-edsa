{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "935ac907",
   "metadata": {},
   "source": [
    "# Unsupervised Learning Predict\n",
    "Â© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "*   Thomas Kenyon\n",
    "*   Name\n",
    "*   Name\n",
    "*   Name\n",
    "*   Name\n",
    "---\n",
    "\n",
    "### Honour Code\n",
    "\n",
    "We, Team 8, confirm - by submitting this document - that the solutions in this notebook are a result of our own work and that we abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "---\n",
    "\n",
    "### Predict Overview: Movie Recommender\n",
    "\n",
    "--Insert description and context here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6cb710",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>\n",
    "\n",
    "<a href=#eight>8. Appendix</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d2a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our regular old heroes \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp # <-- The sister of Numpy, used in our code for numerical efficientcy. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Entity featurization and similarity computation\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Libraries used during sorting procedures.\n",
    "import operator # <-- Convienient item retrieval during iteration \n",
    "import heapq # <-- Efficient sorting of large lists\n",
    "\n",
    "# Imported for our sanity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62474d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_scores = pd.read_csv('genome_scores.csv')\n",
    "genome_tags = pd.read_csv('genome_tags.csv')\n",
    "imdb_data = pd.read_csv('imdb_data.csv')\n",
    "links = pd.read_csv('links.csv')\n",
    "movies = pd.read_csv('movies.csv')\n",
    "tags = pd.read_csv('tags.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1201ce",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Data Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "\n",
    "Prior to training a model with any sort of data, it is essential to re-engineer it. This is to ensure that the data is in a consistent form with no missing values, incorrect data types or just plain incorrect data. For structured,  numeric data, this entails scaling values, filling in missing values and typecasting any non-numeric data to numeric form. Non-numeric data, such as this database is often unstructured and consists of text data. This type of data is not easily interpretable by computers. Computers work with 1s and 0s, not letters and words. Therefore it must be converted into a form that is interpretable, and therefore, numeric data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6223bc",
   "metadata": {},
   "source": [
    "### Content-based Recommender\n",
    "\n",
    "---\n",
    "First thing, we need to merge the movies dataframe with the imdb_data dataframe, these two dataframes contain most of the information we need to construct our recommender.\n",
    "\n",
    "As you can see, for each movie we now have multiple columns that contain useful information about each movie:\n",
    "* genres\n",
    "* title_cast\n",
    "* director\n",
    "* plot_keywords\n",
    "\n",
    "There are more columns that could be useful, such as budget, but lets stick to these for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined2 = movies.merge(imdb_data,  how='left', on = 'movieId')\n",
    "joined2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13eaf76",
   "metadata": {},
   "source": [
    "There is a lot of actors names for each movie in the title_cast column! Since we're going to ultimately be vectorizing all of this data, we should ask ourselves if we really need all the actors for each movie listed in this column. For the sake of minimizing the number of features in our vectorized dataset, lets reduce this to the top 3 actors. The first 3 actors in this column seem to be the top-billed/most prominent cast for each film, so it's likely that they're the most important anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff637478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_3(actors):\n",
    "  s = []\n",
    "  s = [actor for actor in actors if len(s) < 3]\n",
    "  s2 = s[:3]\n",
    "  return '|'.join([actor for actor in s2])\n",
    "\n",
    "# x = ['Rhys Ifans', 'Tim Allen', 'Don Rickles', 'Crispin Glover', 'Christian McKay']\n",
    "# print(first_3(x))\n",
    "\n",
    "\n",
    "joined2['title_cast'] = joined2['title_cast'].apply(lambda x: str(x).split('|'))\n",
    "joined2['title_cast'] = joined2['title_cast'].apply(first_3)\n",
    "joined2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99def7c7",
   "metadata": {},
   "source": [
    "One problem with this dataset are the number of nan values. Most of these are actual np.nan null values, however some of them are actually 'nan' strings. Sneaky! We'll need to convert all these cryptic nan values to real nan values and then imput something whenever they occur, since null values cannot be passed to a vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7ed8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan(field):\n",
    "  '''Some NaN values are being stored as strings, so fillna wont work'''\n",
    "  if not isinstance(field, str):\n",
    "    return np.nan\n",
    "  f2 = field.strip().lower()\n",
    "  if 'nan' in f2:\n",
    "    return np.nan\n",
    "  else:\n",
    "    return field\n",
    "\n",
    "joined3 = joined2.copy()\n",
    "joined3['title_cast'] = joined3['title_cast'].apply(check_nan)\n",
    "joined3['plot_keywords'] = joined3['plot_keywords'].apply(check_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b18444",
   "metadata": {},
   "source": [
    "Lets replace all nan values with empty strings. This means that there will be a lot of movies with a lot of empty metadata columns, but hopefully when we include the genome tag dataset this will become less of an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9529693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined3['director'].fillna('', inplace = True)\n",
    "joined3['title_cast'].fillna('', inplace = True)\n",
    "joined3['runtime'].fillna(0, inplace = True)\n",
    "joined3['budget'].fillna(0, inplace = True)\n",
    "joined3['plot_keywords'].fillna('', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a70c27",
   "metadata": {},
   "source": [
    "Creating a word soup column. This column has all the text data in it that we will vectorize. Think of all the words/names in these columns becoming tags. I've added director 3 times so that it has more weighting. IE, if someone likes a movie like Inception, then they are likely to enjoy other movies directer by Christopher Nolan. This is quite a janky way of altering the weightings of specific tags, but creating a word soup like this does simplify the vectorizing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa34481",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined3['soup'] = joined3['genres'] + '|' + joined3['director'] + '|' + joined3['director'] + '|' + joined3['director'] + '|' + joined3['plot_keywords'] + '|'+ joined3['title_cast']\n",
    "joined3.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad1ee12",
   "metadata": {},
   "source": [
    "The movie titles contain their release year. I remove that and put the years in their own column using the two cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf92959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year(title):\n",
    "  s = title.split()\n",
    "  year = s[-1]\n",
    "  if '(' in year and ')' in year:\n",
    "    year2 = year[1:-1]\n",
    "    return year2\n",
    "  else:\n",
    "    return np.nan\n",
    "  \n",
    "joined3['year'] = joined3['title'].apply(get_year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
